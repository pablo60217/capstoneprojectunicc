{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-9Ppg-VFRYllRdGwCqqmpPyI7l1hNy2T","timestamp":1748632088751}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Import"],"metadata":{"id":"q3qMxYOtM1dq"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from IPython.display import display\n","from google.colab import drive"],"metadata":{"id":"KF1dwjF98iNR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Download Data Sets"],"metadata":{"id":"nREVKp5o77aD"}},{"cell_type":"markdown","source":["#All paths\n"],"metadata":{"id":"F3dizRvy8Kyh"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xsagYxaY8s1U","executionInfo":{"status":"ok","timestamp":1748708413699,"user_tz":-120,"elapsed":939,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"136b000f-21b9-4daa-e84f-5c2b5c56dd6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#Idividual personal path to bring the Data\n","\n","#Juan's persona path\n","#personal_path= \"/content/drive/MyDrive/Term 3/Capstone Project\"\n","\n","#Gizela's personal path\n","personal_path= \"/content/drive/MyDrive/UNICC - Thesis MBD/DataSets\""],"metadata":{"id":"uh43vawOxvVP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the base paths\n","general_path = \"UNICC - Thesis MBD/DataSets\"  # sin slash inicial\n","\n","# Define dataset relative paths\n","datasets = {\n","    # EDOS\n","    \"edos_labelled_aggregated\": \"EDOS/edos_labelled_aggregated.csv\",\n","    \"edos_labelled_individual_annotations\": \"EDOS/edos_labelled_individual_annotations.csv\",\n","\n","    # Hateval 2019\n","    \"hateval2019_es_train\": \"Multilingual Datasets/hateval2019/hateval2019_es_train.csv\",\n","    \"hateval2019_en_test\": \"Multilingual Datasets/hateval2019/hateval2019_en_test.csv\",\n","    \"hateval2019_es_test\": \"Multilingual Datasets/hateval2019/hateval2019_es_test.csv\",\n","    \"hateval2019_es_dev\": \"Multilingual Datasets/hateval2019/hateval2019_es_dev.csv\",\n","    \"hateval2019_en_train\": \"Multilingual Datasets/hateval2019/hateval2019_en_train.csv\",\n","    \"hateval2019_en_dev\": \"Multilingual Datasets/hateval2019/hateval2019_en_dev.csv\",\n","\n","    # ManRoSexism\n","    \"targetResultFile_full2\": \"Multilingual Datasets/ManRoSexism_Twitter_MeTwo/targetResultFile_full2.txt\",\n","\n","    # EXIST\n","    \"EXIST2021_test\": \"Multilingual Datasets/EXIST/EXIST2021_test.tsv.txt\",\n","    \"EXIST2021_training\": \"Multilingual Datasets/EXIST/EXIST2021_training.tsv.txt\",\n","\n","\n","}\n","\n","# Load datasets into a dictionary\n","loaded_data = {}\n","\n","for name, rel_path in datasets.items():\n","    full_path = os.path.join(personal_path, general_path, rel_path)\n","\n","\n","    try:\n","        if name == \"targetResultFile_full2\":\n","            # Carga espec√≠fica para ese archivo con delimitador ;\n","            df = pd.read_csv(full_path, sep=\";\", header=None, names=[\"id\", \"text\", \"label\"], on_bad_lines='skip', engine='python')\n","        elif full_path.endswith(\".csv\"):\n","            df = pd.read_csv(full_path)\n","        elif full_path.endswith(\".tsv.txt\"):\n","            df = pd.read_csv(full_path, sep=\"\\t\")\n","        elif full_path.endswith(\".xlsx\"):\n","            df = pd.read_excel(full_path)\n","        elif full_path.endswith(\".txt\"):\n","            df = pd.read_csv(full_path, delimiter=\"\\t\", engine=\"python\", on_bad_lines='skip')\n","        else:\n","            print(f\"[WARNING] Unknown format for file: {name}\")\n","            continue\n","\n","            # üîÅ Convertir 'id' a texto si existe\n","        if 'id' in df.columns:\n","            df['id'] = df['id'].astype(str)\n","\n","        # A√±adir columnas: source_dataset y split_type\n","        df['source_dataset'] = name\n","        lower_name = name.lower()\n","        if \"train\" in lower_name:\n","            split = \"train\"\n","        elif \"test\" in lower_name:\n","            split = \"test\"\n","        elif \"dev\" in lower_name:\n","            split = \"dev\"\n","        else:\n","            split = \"unknown\"\n","        df['split_type'] = split\n","\n","        loaded_data[name] = df\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Failed to load {name} from {full_path}: {e}\")\n","\n","print(\"\\n--- Estado de carga de los datasets ---\")\n","for name, rel_path in datasets.items():\n","    if name in loaded_data:\n","        df = loaded_data[name]\n","        print(f\"[OK] {name}: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n","    else:\n","        print(f\"[FAILED] {name}: no fue cargado\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8sC7qRZ2G-Q","executionInfo":{"status":"ok","timestamp":1748708536177,"user_tz":-120,"elapsed":7,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"9e5786ed-7b5b-4cd5-e0cb-e981e6872004"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ERROR] Failed to load edos_labelled_aggregated from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/EDOS/edos_labelled_aggregated.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/EDOS/edos_labelled_aggregated.csv'\n","[ERROR] Failed to load edos_labelled_individual_annotations from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/EDOS/edos_labelled_individual_annotations.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/EDOS/edos_labelled_individual_annotations.csv'\n","[ERROR] Failed to load hateval2019_es_train from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_es_train.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_es_train.csv'\n","[ERROR] Failed to load hateval2019_en_test from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_en_test.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_en_test.csv'\n","[ERROR] Failed to load hateval2019_es_test from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_es_test.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_es_test.csv'\n","[ERROR] Failed to load hateval2019_es_dev from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_es_dev.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_es_dev.csv'\n","[ERROR] Failed to load hateval2019_en_train from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_en_train.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_en_train.csv'\n","[ERROR] Failed to load hateval2019_en_dev from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_en_dev.csv: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/hateval2019/hateval2019_en_dev.csv'\n","[ERROR] Failed to load targetResultFile_full2 from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/ManRoSexism_Twitter_MeTwo/targetResultFile_full2.txt: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/ManRoSexism_Twitter_MeTwo/targetResultFile_full2.txt'\n","[ERROR] Failed to load EXIST2021_test from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/EXIST/EXIST2021_test.tsv.txt: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/EXIST/EXIST2021_test.tsv.txt'\n","[ERROR] Failed to load EXIST2021_training from /content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/EXIST/EXIST2021_training.tsv.txt: [Errno 2] No such file or directory: '/content/drive/MyDrive/UNICC - Thesis MBD/DataSets/UNICC - Thesis MBD/DataSets/Multilingual Datasets/EXIST/EXIST2021_training.tsv.txt'\n","\n","--- Estado de carga de los datasets ---\n","[FAILED] edos_labelled_aggregated: no fue cargado\n","[FAILED] edos_labelled_individual_annotations: no fue cargado\n","[FAILED] hateval2019_es_train: no fue cargado\n","[FAILED] hateval2019_en_test: no fue cargado\n","[FAILED] hateval2019_es_test: no fue cargado\n","[FAILED] hateval2019_es_dev: no fue cargado\n","[FAILED] hateval2019_en_train: no fue cargado\n","[FAILED] hateval2019_en_dev: no fue cargado\n","[FAILED] targetResultFile_full2: no fue cargado\n","[FAILED] EXIST2021_test: no fue cargado\n","[FAILED] EXIST2021_training: no fue cargado\n"]}]},{"cell_type":"markdown","source":["#Transformaciones de las columnas"],"metadata":{"id":"YWHomPIy60zC"}},{"cell_type":"code","source":["# A√±adir columnas a todos los DataFrames cargados\n","for name, df in loaded_data.items():\n","    # Columna con el nombre del dataset\n","    df['source_dataset'] = name\n","\n","    # EXCEPCI√ìN: asignar 'test' manualmente\n","    if name == \"targetResultFile_full2\":\n","        df['split_type'] = \"test\"\n","\n","    # Si tiene una columna 'split', √∫sala\n","    elif 'split' in df.columns:\n","        unique_splits = df['split'].dropna().unique()\n","        if len(unique_splits) == 1:\n","            df['split_type'] = unique_splits[0].lower()\n","        else:\n","            df['split_type'] = df['split'].astype(str).str.lower()\n","\n","    # Si no, inferir desde el nombre\n","    else:\n","        lower_name = name.lower()\n","        if \"train\" in lower_name:\n","            split = \"train\"\n","        elif \"test\" in lower_name:\n","            split = \"test\"\n","        elif \"dev\" in lower_name:\n","            split = \"dev\"\n","        else:\n","            split = \"unknown\"\n","        df['split_type'] = split\n","\n","    # Guardar nuevamente\n","    loaded_data[name] = df\n","\n","# Resumen\n","print(\"\\n--- Resumen de columnas y tipo de conjunto por dataset ---\\n\")\n","for name, df in loaded_data.items():\n","    columnas = list(df.columns)\n","    split_type_vals = df['split_type'].unique()\n","    print(f\"üóÇÔ∏è  {name}\")\n","    print(f\"   - Columnas: {columnas}\")\n","    print(f\"   - Valores en 'split_type': {split_type_vals}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mW62D9rE3XcW","executionInfo":{"status":"ok","timestamp":1748708543419,"user_tz":-120,"elapsed":8,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"b5c5e041-2ff9-4648-de58-f4bf1f2019ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Resumen de columnas y tipo de conjunto por dataset ---\n","\n"]}]},{"cell_type":"code","source":["# Detectar columna objetivo y copiar sus valores a una nueva columna real\n","target_column_keywords = [\"label\", \"label_sexist\", \"HS\", \"task1\"]\n","\n","print(\"\\n--- Extrayendo valor real de la etiqueta sexista en cada dataset ---\\n\")\n","\n","for name, df in loaded_data.items():\n","    found = None\n","\n","    # Casos personalizados\n","    if name == \"edos_labelled_aggregated\":\n","        found = \"label_sexist\"\n","    elif name == \"edos_labelled_individual_annotations\":\n","        found = \"label_sexist\"\n","    elif name.startswith(\"hateval2019\"):\n","        found = \"HS\"\n","    elif name.startswith(\"EXIST2021\"):\n","        found = \"task1\"\n","    elif name == \"targetResultFile_full2\":\n","        found = \"label\"\n","    else:\n","        # Detecci√≥n autom√°tica\n","        for col in df.columns:\n","            if any(key.lower() in col.lower() for key in target_column_keywords):\n","                found = col\n","                break\n","\n","    if found and found in df.columns:\n","        # Asignamos los valores reales de la columna objetivo\n","        df['target_column'] = df[found]\n","        print(f\"‚úÖ {name}: columna objetivo -> '{found}' copiada como 'target_column'\")\n","    else:\n","        # No se pudo detectar columna v√°lida\n","        df['target_column'] = None\n","        print(f\"‚ö†Ô∏è  {name}: No se encontr√≥ columna objetivo. Se asign√≥ None.\")\n","\n","    # Actualizamos el diccionario\n","    loaded_data[name] = df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"899SVIBb9UEN","executionInfo":{"status":"ok","timestamp":1748708546162,"user_tz":-120,"elapsed":7,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"e69c8da3-7049-421f-d5c0-b159a33fb93a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Extrayendo valor real de la etiqueta sexista en cada dataset ---\n","\n"]}]},{"cell_type":"code","source":["print(\"\\n--- Normalizando valores de 'target_column' a binario (1=sexist, 0=non-sexist) ---\\n\")\n","\n","for name, df in loaded_data.items():\n","    if 'target_column' not in df.columns:\n","        print(f\"‚ö†Ô∏è  {name}: no tiene 'target_column', se omite.\")\n","        continue\n","\n","    original_values = df['target_column'].unique()\n","\n","    if df['target_column'].dtype == 'O':  # tipo texto u objeto\n","        df['target_column'] = df['target_column'].str.lower().map({\n","            'sexist': 1,\n","            'non-sexist': 0,\n","            'non_sexist': 0,\n","            'not sexist': 0,\n","            'no': 0,\n","            'yes': 1,\n","\n","        })\n","    else:\n","        # Asumimos que ya est√° en 0/1 o NaN\n","        df['target_column'] = df['target_column'].map(lambda x: 1 if x == 1 else (0 if x == 0 else None))\n","\n","    # Guardamos de nuevo\n","    loaded_data[name] = df\n","\n","    updated_values = df['target_column'].dropna().unique()\n","    print(f\"‚úÖ {name}: {original_values} ‚Üí normalizado a {updated_values}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HjAFBMYj-BdW","executionInfo":{"status":"ok","timestamp":1748708548442,"user_tz":-120,"elapsed":3,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"52f0f801-6093-4119-9dd4-88fd9559a306"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Normalizando valores de 'target_column' a binario (1=sexist, 0=non-sexist) ---\n","\n"]}]},{"cell_type":"code","source":["#se agrega el 0.5 a targetResultFile_full2 devido a que tiene la categoria DOUBTFUL dentro del label\n","# Obtener el DataFrame\n","df = loaded_data[\"targetResultFile_full2\"]\n","\n","# Mapear valores con inclusi√≥n de 0.5 para 'DOUBTFUL'\n","df['target_column'] = df['label'].str.upper().map({\n","    'SEXIST': 1,\n","    'NON_SEXIST': 0,\n","    'DOUBTFUL': 0.5\n","})\n","\n","# Confirmar cambios\n","print(\"‚úÖ Asignaci√≥n completa en 'target_column':\")\n","print(df['target_column'].value_counts(dropna=False))\n","\n","# Guardar de nuevo en el diccionario\n","loaded_data[\"targetResultFile_full2\"] = df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"L1WLW3mHtNio","executionInfo":{"status":"error","timestamp":1748708550462,"user_tz":-120,"elapsed":12,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"84ea528f-9ec0-48cf-ba77-368c3f3937d4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'targetResultFile_full2'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-7158443c2158>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#se agrega el 0.5 a targetResultFile_full2 devido a que tiene la categoria DOUBTFUL dentro del label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Obtener el DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targetResultFile_full2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Mapear valores con inclusi√≥n de 0.5 para 'DOUBTFUL'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'targetResultFile_full2'"]}]},{"cell_type":"code","source":["print(\"\\n--- A√±adiendo subcategor√≠as a los datasets ---\\n\")\n","\n","for name, df in loaded_data.items():\n","    # Inicializar columnas vac√≠as\n","    df['subcategory_general'] = None\n","    df['subcategory_specific'] = None\n","    df['subcategory_combined'] = None\n","\n","    if name.startswith(\"edos_labelled_\"):\n","        if {'label_category', 'label_vector', 'target_column'}.issubset(df.columns):\n","            # Solo mantener subcategor√≠as si target_column == 1\n","            mask = df['target_column'] == 1\n","            df.loc[mask, 'subcategory_general'] = df.loc[mask, 'label_category'].str.strip()\n","            df.loc[mask, 'subcategory_specific'] = df.loc[mask, 'label_vector'].str.strip()\n","            df.loc[mask, 'subcategory_combined'] = (\n","                df.loc[mask, 'target_column'].astype(str) + \" | \" +\n","                df.loc[mask, 'subcategory_general'] + \" | \" +\n","                df.loc[mask, 'subcategory_specific']\n","            )\n","            print(f\"‚úÖ {name}: subcategor√≠as extra√≠das de 'label_category' y 'label_vector'\")\n","        else:\n","            print(f\"‚ö†Ô∏è  {name}: faltan columnas 'label_category', 'label_vector' o 'target_column'\")\n","\n","    elif name.startswith(\"hateval2019\") and {'TR', 'AG', 'target_column'}.issubset(df.columns):\n","        # Solo mantener subcategor√≠as si target_column == 1\n","        mask = df['target_column'] == 1\n","        df.loc[mask, 'subcategory_general'] = df.loc[mask, 'TR'].map({1: \"targeted\", 0: \"untargeted\"})\n","        df.loc[mask, 'subcategory_specific'] = df.loc[mask, 'AG'].map({1: \"aggressive\", 0: \"non-aggressive\"})\n","        df.loc[mask, 'subcategory_combined'] = (\n","            df.loc[mask, 'target_column'].astype(str) + \" | \" +\n","            df.loc[mask, 'subcategory_general'] + \" | \" +\n","            df.loc[mask, 'subcategory_specific']\n","        )\n","        print(f\"‚úÖ {name}: subcategor√≠as construidas desde 'TR' y 'AG'\")\n","\n","    else:\n","        print(f\"‚ÑπÔ∏è  {name}: no tiene subcategor√≠as disponibles, columnas quedan vac√≠as\")\n","\n","    loaded_data[name] = df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fyu87kGsAr50","executionInfo":{"status":"ok","timestamp":1748707872171,"user_tz":-120,"elapsed":4,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"3d242f60-a98d-44e9-f290-0a9a789a74c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- A√±adiendo subcategor√≠as a los datasets ---\n","\n"]}]},{"cell_type":"code","source":["# Columnas creadas que queremos analizar\n","columnas_objetivo = [\n","    \"target_column\",\n","    \"subcategory_general\",\n","    \"subcategory_specific\",\n","    \"subcategory_combined\"\n","]\n","\n","print(\"\\n--- An√°lisis completo de columnas clave por dataset ---\\n\")\n","\n","for name, df in loaded_data.items():\n","    total_filas = len(df)\n","    print(f\"üìò Dataset: {name}\")\n","    print(f\"   Total de filas: {total_filas}\")\n","\n","    for col in columnas_objetivo:\n","        if col not in df.columns:\n","            print(f\"   ‚ö†Ô∏è  {col} no existe en este dataset\")\n","            continue\n","\n","        n_nulos = df[col].isna().sum()\n","        pct_nulos = (n_nulos / total_filas) * 100\n","        unicos = df[col].nunique(dropna=True)\n","\n","        print(f\"   üîπ {col}:\")\n","        print(f\"      - Nulos: {n_nulos} ({pct_nulos:.2f}%)\")\n","        print(f\"      - Valores √∫nicos (no nulos): {unicos}\")\n","\n","        # An√°lisis especial para target_column\n","        if col == \"target_column\":\n","            n_1 = (df[col] == 1).sum()\n","            n_0 = (df[col] == 0).sum()\n","            n_05 = (df[col] == 0.5).sum()\n","            pct_1 = (n_1 / total_filas) * 100\n","            pct_0 = (n_0 / total_filas) * 100\n","            pct_05 = (n_05 / total_filas) * 100\n","            print(f\"      - Valor 1 (sexista): {n_1} ({pct_1:.2f}%)\")\n","            print(f\"      - Valor 0 (no sexista): {n_0} ({pct_0:.2f}%)\")\n","            print(f\"      - Valor 0.5 (doubtful): {n_05} ({pct_05:.2f}%)\")\n","\n","    print(\"-\" * 100)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mj3QfjZNn5kX","executionInfo":{"status":"ok","timestamp":1748707875494,"user_tz":-120,"elapsed":8,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"547da402-1394-46f9-930f-6bb5d4509468"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- An√°lisis completo de columnas clave por dataset ---\n","\n"]}]},{"cell_type":"code","source":["print(\"\\n--- Vista resumida de los datasets cargados (muestras aleatorias) ---\\n\")\n","\n","for name, df in loaded_data.items():\n","    print(f\"üìò Dataset: {name}\")\n","    print(f\"üîπ Shape: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n","\n","    # Mostrar solo las primeras 15 columnas si hay muchas\n","    max_cols = 15\n","    cols_to_show = df.columns[:max_cols] if df.shape[1] > max_cols else df.columns\n","\n","    # Verifica que tenga suficientes filas para samplear\n","    sample_size = min(5, df.shape[0])\n","    try:\n","        sample = df[cols_to_show].sample(sample_size, random_state=42)\n","    except ValueError:\n","        sample = df[cols_to_show]  # en caso de que samplee m√°s de lo que hay\n","\n","    print(f\"(Mostrando {sample_size} filas aleatorias)\")\n","    display(sample)\n","    print(\"-\" * 100)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfCRniLq9qRj","executionInfo":{"status":"ok","timestamp":1748707782681,"user_tz":-120,"elapsed":6,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"81616497-7652-481a-818b-62bb7f326065"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Vista resumida de los datasets cargados (muestras aleatorias) ---\n","\n"]}]},{"cell_type":"markdown","source":["#Union de los Data Sets"],"metadata":{"id":"6gxXLBc4Mryr"}},{"cell_type":"code","source":["# Paso 1: unir todos los datasets en un solo DataFrame\n","df_unificado = pd.concat(loaded_data.values(), ignore_index=True)\n","\n","# Paso 2: columnas prioritarias\n","columnas_principales = [\n","    \"source_dataset\", \"target_column\", \"subcategory_combined\",\n","    \"text\", \"id\", \"subcategory_general\", \"subcategory_specific\"\n","]\n","\n","# Paso 3: detectar todas las columnas √∫nicas del conjunto\n","todas_las_columnas = list(df_unificado.columns)\n","\n","# Paso 4: ordenar columnas\n","otras_columnas = [col for col in todas_las_columnas if col not in columnas_principales]\n","orden_final = columnas_principales + otras_columnas\n","\n","# Paso 5: reordenar columnas\n","df_unificado = df_unificado[orden_final]\n","\n","\n","\n","# Asegurar que pandas muestre todas las columnas (sin \"...\")\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)  # en notebooks/jupyter\n","\n","# Muestra aleatoria y ordenada\n","num_muestras = min(50, df_unificado.shape[0])\n","muestra_ordenada = df_unificado.sample(num_muestras, random_state=42).sort_values(by='source_dataset')\n","\n","print(f\"\\nüìä Mostrando {num_muestras} filas aleatorias ordenadas por 'source_dataset':\\n\")\n","display(muestra_ordenada)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"gWQWFkjKIbwf","executionInfo":{"status":"error","timestamp":1748707878057,"user_tz":-120,"elapsed":30,"user":{"displayName":"Gizela Susan Thomas","userId":"11401285322692095583"}},"outputId":"160ff0ac-dc04-4c97-9354-060a83e19416"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"No objects to concatenate","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-88222358223c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Paso 1: unir todos los datasets en un solo DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_unificado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Paso 2: columnas prioritarias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m columnas_principales = [\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_keys_and_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m_clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No objects to concatenate"]}]},{"cell_type":"markdown","source":["#Guardar el archivo\n","\n","Se usa el formato Pickle para conservar todos los parametros."],"metadata":{"id":"Lv8cTNWxPXWP"}},{"cell_type":"code","source":["# Ruta completa para guardar el archivo\n","output_pickle_path = os.path.join(personal_path, general_path, \"dataset_unificado.pkl\")\n","\n","# Guardar como Pickle\n","df_unificado.to_pickle(output_pickle_path)\n","\n","print(f\"‚úÖ Dataset guardado como Pickle en:\\n{output_pickle_path}\")\n","print(f\"üîπ Filas: {df_unificado.shape[0]}, Columnas: {df_unificado.shape[1]}\")\n","\n","# Verificar si el archivo fue guardado y se puede cargar\n","if os.path.exists(output_pickle_path):\n","    try:\n","        df_verificacion = pd.read_pickle(output_pickle_path)\n","        print(\"‚úÖ Archivo cargado correctamente. Revisi√≥n r√°pida:\")\n","        print(f\"üîπ Filas: {df_verificacion.shape[0]}, Columnas: {df_verificacion.shape[1]}\")\n","        print(f\"üîπ Columnas clave: {', '.join([c for c in ['source_dataset', 'target_column', 'text'] if c in df_verificacion.columns])}\")\n","    except Exception as e:\n","        print(f\"‚ùå Error al intentar cargar el archivo: {e}\")\n","else:\n","    print(\"‚ùå El archivo no fue encontrado despu√©s de guardarlo.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ar35QOVAOi-N","executionInfo":{"status":"ok","timestamp":1748634711914,"user_tz":-120,"elapsed":314,"user":{"displayName":"Juan Echeverri","userId":"00081912204900330630"}},"outputId":"9749509e-f879-4ac7-aed9-cd626691c914"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Dataset guardado como Pickle en:\n","/content/drive/MyDrive/Term 3/Capstone Project/UNICC - Thesis MBD/DataSets/dataset_unificado.pkl\n","üîπ Filas: 113416, Columnas: 23\n","‚úÖ Archivo cargado correctamente. Revisi√≥n r√°pida:\n","üîπ Filas: 113416, Columnas: 23\n","üîπ Columnas clave: source_dataset, target_column, text\n"]}]}]}